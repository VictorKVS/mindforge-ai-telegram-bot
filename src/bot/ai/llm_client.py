"""
LLM Client module: supports both mock LLM (for tests)
and Local LLaMA model via llama-cpp-python.
"""

from llama_cpp import Llama
from src.bot.config import settings


# -------------------------------------------------------------------
# 1. MOCK CLIENT (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)
# -------------------------------------------------------------------
class LLMClient:
    """Mock LLM client for tests."""

    def get_context(self, query: str):
        # –¢–µ—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        return ["Mock context"]

    def generate(self, prompt: str):
        # –¢–µ—Å—Ç—ã –æ–∂–∏–¥–∞—é—Ç —Å—Ç—Ä–æ–≥–æ —Ç–∞–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–∞
        context = self.get_context(prompt)[0]
        return f"LLM response to: {prompt}\nContext: {context}"


# -------------------------------------------------------------------
# 2. Local LLaMA Client
# -------------------------------------------------------------------
class LocalLLMClient:
    """Local LLaMA model using llama-cpp-python (GGUF)."""

    def __init__(self):
        print("üî• Local LLaMA loading...")

        self.model = Llama(
            model_path=settings.LOCAL_LLM_MODEL_PATH,
            n_ctx=settings.LOCAL_LLM_CTX,
            n_threads=settings.LOCAL_LLM_THREADS,
            n_gpu_layers=settings.LOCAL_LLM_GPU_LAYERS,
            verbose=False,
        )

        print("‚úÖ Local LLaMA loaded!")

    async def generate(self, prompt: str) -> str:
        """Generate text using the local LLaMA model."""
        output = self.model(
            prompt,
            max_tokens=256,
            temperature=0.7,
        )

        return output["choices"][0]["text"].strip()


# -------------------------------------------------------------------
# 3. Factory ‚Äî –≤—ã–±–∏—Ä–∞–µ–º –∫–ª–∏–µ–Ω—Ç–∞
# -------------------------------------------------------------------
def get_llm_client():
    """
    –í—ã–±–∏—Ä–∞–µ—Ç –º–æ–¥–µ–ª—å:
      - –µ—Å–ª–∏ LOCAL_LLM_ENABLED = true ‚Üí LocalLLM
      - –∏–Ω–∞—á–µ ‚Üí Mock LLM (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)
    """
    if getattr(settings, "LOCAL_LLM_ENABLED", False):
        return LocalLLMClient()

    return LLMClient()


# –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ –≤—Å–µ–º—É –ø—Ä–æ–µ–∫—Ç—É
llm_client = get_llm_client()
