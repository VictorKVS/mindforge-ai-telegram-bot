from aiogram import Router, types
from src.bot.ai.llm_router import LLMRouter

router = Router()
llm_router = LLMRouter()


# ----------------------------------------------------------
# /model ‚Äî help
# ----------------------------------------------------------

@router.message(commands=["model"])
async def model_help(message: types.Message):
    await message.answer(
        "üì° *LLM Model Manager*\n\n"
        "/model list ‚Äì —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n"
        "/model current ‚Äì –∞–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å\n"
        "/model set <–∏–º—è> ‚Äì –∏–∑–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å\n"
        "/model health ‚Äì –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n"
        "/model metrics ‚Äì –º–µ—Ç—Ä–∏–∫–∏ —Ä–æ—É—Ç–µ—Ä–∞\n",
        parse_mode="Markdown"
    )


# ----------------------------------------------------------
# /model list
# ----------------------------------------------------------

@router.message(commands=["modellist"])
async def model_list(message: types.Message):
    providers = llm_router.list_providers()
    active = llm_router.active

    text = "ü§ñ *Available LLM Providers:*\n\n"
    for p in providers:
        marker = "üü¢" if p == active else "‚ö™"
        text += f"{marker} `{p}`\n"

    await message.answer(text, parse_mode="Markdown")


# ----------------------------------------------------------
# /model current
# ----------------------------------------------------------

@router.message(commands=["modelcurrent"])
async def model_current(message: types.Message):
    await message.answer(
        f"üîç Current LLM Provider: *{llm_router.active}*",
        parse_mode="Markdown"
    )


# ----------------------------------------------------------
# /model set <name>
# ----------------------------------------------------------

@router.message(commands=["modelset"])
async def model_set(message: types.Message):
    args = message.text.split()

    if len(args) < 2:
        await message.answer("‚ö† –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: /modelset <–∏–º—è_–º–æ–¥–µ–ª–∏>")
        return

    provider = args[1]

    try:
        llm_router.set_default(provider)
        await message.answer(f"‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∞ –Ω–∞: *{provider}*", parse_mode="Markdown")

    except ValueError:
        await message.answer("‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /modellist")


# ----------------------------------------------------------
# /model health
# ----------------------------------------------------------

@router.message(commands=["modelhealth"])
async def model_health(message: types.Message):
    info = llm_router.health_check()

    text = "ü©∫ *LLM Health Status*\n\n"

    for p, st in info.items():
        if st["status"] == "healthy":
            text += f"üü¢ {p}: OK\n"
        else:
            text += f"üî¥ {p}: ERROR ‚Äì `{st.get('error', 'unknown')}`\n"

    await message.answer(text, parse_mode="Markdown")


# ----------------------------------------------------------
# /model metrics
# ----------------------------------------------------------

@router.message(commands=["modelmetrics"])
async def model_metrics(message: types.Message):
    m = llm_router.get_metrics()

    text = (
        "üìä *LLM Router Metrics*\n\n"
        f"Total Requests: {m['total_requests']}\n"
        f"Errors: {m['error_count']}\n"
        f"Success Rate: {m['success_rate']:.2%}\n"
        f"Avg Latency: {m['avg_latency']:.3f} sec\n"
    )

    await message.answer(text, parse_mode="Markdown")
